{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "supreme-silicon",
   "metadata": {},
   "source": [
    "### Data Set Information:\n",
    "https://archive.ics.uci.edu/ml/datasets/Bank+Marketing#\n",
    "\n",
    "The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed. \n",
    "\n",
    "There are four datasets: \n",
    "1) bank-additional-full.csv with all examples (41188) and 20 inputs, ordered by date (from May 2008 to November 2010), very close to the data analyzed in [Moro et al., 2014]\n",
    "2) bank-additional.csv with 10% of the examples (4119), randomly selected from 1), and 20 inputs.\n",
    "3) bank-full.csv with all examples and 17 inputs, ordered by date (older version of this dataset with less inputs). \n",
    "4) bank.csv with 10% of the examples and 17 inputs, randomly selected from 3 (older version of this dataset with less inputs). \n",
    "The smallest datasets are provided to test more computationally demanding machine learning algorithms (e.g., SVM). \n",
    "\n",
    "The classification goal is to predict if the client will subscribe (yes/no) a term deposit (variable y).\n",
    "\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "Input variables:\n",
    "* Bank client data:\n",
    "* 1 - age (numeric)\n",
    "* 2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
    "* 3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
    "* 4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n",
    "* 5 - default: has credit in default? (categorical: 'no','yes','unknown')\n",
    "* 6 - housing: has housing loan? (categorical: 'no','yes','unknown')\n",
    "* 7 - loan: has personal loan? (categorical: 'no','yes','unknown')\n",
    "* Related with the last contact of the current campaign:\n",
    "* 8 - contact: contact communication type (categorical: 'cellular','telephone') \n",
    "* 9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
    "* 10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n",
    "* 11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "* Other attributes:\n",
    "v12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "* 13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "* 14 - previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "* 15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
    "* Social and economic context attributes\n",
    "* 16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\n",
    "* 17 - cons.price.idx: consumer price index - monthly indicator (numeric) \n",
    "* 18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric) \n",
    "* 19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n",
    "* 20 - nr.employed: number of employees - quarterly indicator (numeric)\n",
    "\n",
    "Output variable (desired target):\n",
    "* 21 - y - has the client subscribed a term deposit? (binary: 'yes','no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-disney",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import brewer2mpl\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# from matplotlib import rcParamss\n",
    "# plt.style.use('ggplot')\n",
    "# plt.style.use('seaborn-colorblind')\n",
    "# plt.rcParams.update({'font.size': 10})\n",
    "%matplotlib inline\n",
    "# np.random.seed(0)\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-technology",
   "metadata": {},
   "outputs": [],
   "source": [
    "#colorbrewer2 Dark2 qualitative color table\n",
    "dark2_cmap = brewer2mpl.get_map('Dark2', 'Qualitative', 7)\n",
    "dark2_colors = dark2_cmap.mpl_colors\n",
    "\n",
    "rcParams['figure.figsize'] = (10, 6)\n",
    "rcParams['figure.dpi'] = 150\n",
    "# rcParams['axes.color_cycle'] = dark2_colors\n",
    "rcParams['lines.linewidth'] = 2\n",
    "rcParams['axes.facecolor'] = 'white'\n",
    "rcParams['font.size'] = 14\n",
    "rcParams['patch.edgecolor'] = 'white'\n",
    "rcParams['patch.facecolor'] = dark2_colors[0]\n",
    "rcParams['font.family'] = 'StixGeneral'\n",
    "\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-aquatic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column to numbers\n",
    "def pd_column_to_number(df,col_name):\n",
    "    \"\"\"\n",
    "    Convert number in strings to number\n",
    "\n",
    "    Args:\n",
    "        df(dataframe): a pandas dataframe to perform the conversion on\n",
    "        col_name (list): a list of column headers\n",
    "    Returns:\n",
    "        df: dataframe with numbers\n",
    "    \"\"\"\n",
    "    \n",
    "    for c in col_name:\n",
    "        df[c] = [string_to_number(x) for x in df[c]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a number in accounting format from string to float\n",
    "def string_to_number(s):\n",
    "    \"\"\"\n",
    "    Convert number in accounting format from string to float.\n",
    "\n",
    "    Args:\n",
    "        s: number as string in accounting format\n",
    "    Returns:\n",
    "        float number\n",
    "    \"\"\"\n",
    "\n",
    "    if type(s).__name__==\"str\":\n",
    "        s = s.strip()\n",
    "        if s ==\"-\":\n",
    "            s = 0\n",
    "        else:\n",
    "            s = s.replace(\",\",\"\").replace(\"$\",\"\")\n",
    "            if s.find(\"(\")>=0 and s.find(\")\")>=0:\n",
    "                s = s.replace(\"(\",\"-\").replace(\")\",\"\")\n",
    "    return float(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers(DataFrame,Series):\n",
    "    iqr = Series.quantile(.75) - Series.quantile(.25)\n",
    "    lower_bound = Series.quantile(.25) - (1.5*iqr)\n",
    "    upper_bound = Series.quantile(.75) + (1.5*iqr)\n",
    "    return DataFrame[(Series >= upper_bound) | (Series <= lower_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_state(df, cols, legend, color='YlGn'):\n",
    "    \"\"\" \n",
    "    Heat-map to plot variable by the U.S. states.\n",
    "    \n",
    "    Args:\n",
    "        df(dataframe): pandas dataframe that contains the data to plot\n",
    "        col(list): the col name for which the map should be plotted, col[0] contains states code \n",
    "        legend(str): legend to use \n",
    "        color(str): color theme to use (see https://github.com/dsc/colorbrewer-python)\n",
    "    Returns:\n",
    "        Folium interactive map\n",
    "    \"\"\"\n",
    "\n",
    "    url = 'https://raw.githubusercontent.com/python-visualization/folium/master/examples/data'\n",
    "    state_geo = f'{url}/us-states.json'\n",
    "\n",
    "    m = folium.Map(location=[48, -102], zoom_start=3)\n",
    "    folium.Choropleth(\n",
    "        geo_data=state_geo,\n",
    "        name='choropleth',\n",
    "        data=df,\n",
    "        columns=cols,\n",
    "        key_on='feature.id',\n",
    "        fill_color=color,\n",
    "        fill_opacity=0.7,\n",
    "        line_opacity=0.2,\n",
    "        legend_name=legend\n",
    "    ).add_to(m)\n",
    "    folium.LayerControl().add_to(m)\n",
    "    return m\n",
    "\n",
    "\n",
    "### Plot loan default rates by states\n",
    "df_state = df.groupby('State').mean()['Default']*100\n",
    "columns =['State', 'Default_Rate']\n",
    "heatmap_state(df_state, columns, 'Default Rate (%)', 'YlOrRd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/stevalang/Galvanize/0002_capstones/capstone1/data/bank/bank-full.csv', delimiter=';')\n",
    "# df = pd.read_csv('/Users/stevalang/Galvanize/0002_capstones/capstone1/data/bank/bank-full.csv', delimiter=';',\n",
    "#                 na_values = 'unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-poster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the first five rows of the bank-full.csv file. \n",
    "# I can see a handful of unknown data already!\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-seafood",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-education",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-count",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_item = []\n",
    "for col in df.columns:\n",
    "    list_item.append([col, df[col].dtype, df[col].isna().sum(), round((df[col].isna().sum()/len(df[col]))*100,2),\n",
    "                      df[col].nunique(), list(df[col].sample(5).drop_duplicates().values)])\n",
    "\n",
    "dfDesc = pd.DataFrame(columns=['feature', 'data_type', 'null', 'nulPct', 'unique', 'uniqueSample'],data=list_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDesc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Unique values of all the column\n",
    "# for x in df[['age', 'job','marital','education','default','balance','housing','loan','contact','day',\n",
    "#              'month','duration','campaign', 'campaign','pdays', 'previous','poutcome', 'y']].columns:\n",
    "#     print(f'{x}: \\n{df[x].unique()}\\n')\n",
    "\n",
    "for i in df.columns:\n",
    "    print(i)\n",
    "    print(df[i].unique())\n",
    "    print('----'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_item = []\n",
    "for col in df.columns:\n",
    "    list_item.append([col, df[col].dtype, df[col].isna().sum(), round((df[col].isna().sum()/len(df[col]))*100,2),\n",
    "                      df[col].nunique(), list(df[col].sample(5).drop_duplicates().values)])\n",
    "\n",
    "dfDesc = pd.DataFrame(columns=['feature', 'data_type', 'null', 'nulPct', 'unique', 'uniqueSample'],data=list_item)\n",
    "dfDesc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-portable",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of non numeric columns\n",
    "numerical_cols = list(df.select_dtypes(exclude=['object']))\n",
    "numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-attribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Variables:\n",
    "category_cols = list(df.select_dtypes(include=['object']))\n",
    "category_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-northwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of missing data points per column\n",
    "# missing_values_count = df.isnull().sum()\n",
    "# missing_values_count = df.isna().sum()\n",
    "# missing_values_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many total missing values do we have?\n",
    "# total_cells = np.product(df.shape)\n",
    "# total_missing = missing_values_count.sum()\n",
    "# total_missing\n",
    "# df.isnull().values.any()\n",
    "# percent of data that is missing\n",
    "# percent_missing = (total_missing/total_cells) * 100\n",
    "# percent_missing\n",
    "# df.dropna()\n",
    "\n",
    "\n",
    "# get the number of missing data points per column\n",
    "missing_values_count = df.isnull().sum()\n",
    "print(missing_values_count)\n",
    "# how many total missing values do we have?\n",
    "total_cells = np.product(df.shape)\n",
    "total_missing = missing_values_count.sum()\n",
    "\n",
    "# percent of data that is missing\n",
    "percent_missing = (total_missing/total_cells) * 100\n",
    "round(percent_missing, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just how much data did we lose?\n",
    "# print(\"Columns in original dataset: %d \\n\" % nfl_data.shape[1])\n",
    "# print(\"Columns with na's dropped: %d\" % columns_with_na_dropped.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-climate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = df.contact == 'cellular'\n",
    "# df = df.drop('contact',axis=1)\n",
    "# result.mean()\n",
    "# df[ df.contact == 'cellular' ] = 'yes'\n",
    "# df[ df.contact == 'telephone' ] = 'yes'\n",
    "\n",
    "# result = df.contact == 'telephone'\n",
    "# result.mean()\n",
    "\n",
    "# result = df.contact == 'unknown'\n",
    "# result.mean()\n",
    "\n",
    "# df.contact.unique() # array(['unknown', 'cellular', 'telephone'], dtype=object)\n",
    "# contact_col = df['contact'].copy()\n",
    "# contact_col[df.contact == 'unknown'] = 'no'\n",
    "# contact_col[ df.contact == 'telephone' ] = 'yes'\n",
    "# contact_col[ df.contact == 'cellular' ] = 'yes'\n",
    "\n",
    "# df.contact = contact_col\n",
    "# df.job.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-footage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-charles",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.job.count('management')\n",
    "df[ df.job == 'admin.' ] = 'admin'\n",
    "managment = (df.job == 'management').mean()\n",
    "technician = (df.job == 'technician').mean()\n",
    "entrepreneur = (df.job == 'entrepreneur').mean()\n",
    "blue_collar = (df.job == 'blue-collar').mean()\n",
    "retired = (df.job == 'retired').mean()\n",
    "admin = (df.job == 'admin').mean()\n",
    "services = (df.job == 'services').mean()\n",
    "unemployed = (df.job == 'unemployed').mean()\n",
    "self_employed = (df.job == 'self-employed').mean()\n",
    "housemaid = (df.job == 'housemaid').mean()\n",
    "student = (df.job == 'student').mean()\n",
    "unknown = (df.job == 'unknown').mean()\n",
    "\n",
    "management = df.job.value_counts()['management']\n",
    "technician = df.job.value_counts()['technician']\n",
    "entrepreneur = df.job.value_counts()['entrepreneur']\n",
    "blue_collar = df.job.value_counts()['blue-collar']\n",
    "retired = df.job.value_counts()['retired']\n",
    "admin = df.job.value_counts()['admin']\n",
    "services = df.job.value_counts()['services']\n",
    "unemployed = df.job.value_counts()['unemployed']\n",
    "self_employed = df.job.value_counts()['self-employed']\n",
    "housemaid = df.job.value_counts()['housemaid']\n",
    "student = df.job.value_counts()['student']\n",
    "unknown = df.job.value_counts()['unknown']\n",
    "\n",
    "djobs = {'management':management, 'technician':technician, 'entrepreneur':entrepreneur, 'blue_collar':blue_collar,\n",
    "'retired':retired, 'admin':admin, 'services':services, 'unemployed':unemployed, 'self_employed':self_employed,\n",
    "'housemaid':housemaid, 'student':student, 'unknown':unknown}\n",
    "\n",
    "# djobs.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-profession",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# See what are the categories and are there any missing values for these categories.\n",
    "for col in category_cols:\n",
    "    plt.figure(figsize=(10,6), dpi=150)\n",
    "    sns.barplot(df[col].value_counts().values, df[col].value_counts().index)\n",
    "    plt.title(col)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-vertex",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#List of the normalized relative frequency of the target class per category.\n",
    "#Normalized distribution of each class per feature and plotted the difference between positive and negative frequencies. \n",
    "#Positive values imply this category favors clients that will subscribe and negative values categories that favor not buying the product.\n",
    "\n",
    "for col in category_cols:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    #Returns counts of unique values for each outcome for each feature.\n",
    "    pos_counts = df.loc[df.y.values == 'yes', col].value_counts()\n",
    "    neg_counts = df.loc[df.y.values == 'no' ,  col].value_counts()\n",
    "    \n",
    "    all_counts = list(set(list(pos_counts.index) + list(neg_counts.index)))\n",
    "    \n",
    "    # Counts of how often each outcome was recorded.\n",
    "    freq_pos = (df.y.values == 'yes').sum()\n",
    "    freq_neg = (df.y.values == 'no').sum()\n",
    "    \n",
    "    pos_counts = pos_counts.to_dict()\n",
    "    neg_counts = neg_counts.to_dict()\n",
    "    \n",
    "    \n",
    "    all_index = list(all_counts)\n",
    "    all_counts = [pos_counts.get(k, 0) / freq_pos - neg_counts.get(k, 0) / freq_neg for k in all_counts]\n",
    "    \n",
    "    sns.barplot(all_counts, all_index)\n",
    "    plt.title(col)\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-success",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.default.replace('unknown', 'no', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-resource",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.housing.replace('unknown',df.housing.mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['loan'] = df.default.replace('unknown',df.loan.mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-theta",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['age']>60) & (df['job']=='unknown'), 'job'] = 'retired'\n",
    "df.loc[(df['education']=='unknown') & (df['job']=='management'), 'education'] = 'university.degree'\n",
    "df.loc[(df['education']=='unknown') & (df['job']=='services'), 'education'] = 'high.school'\n",
    "df.loc[(df['education']=='unknown') & (df['job']=='housemaid'), 'education'] = 'basic.4y'\n",
    "df.loc[(df['job'] == 'unknown') & (df['education']=='basic.4y'), 'job'] = 'blue-collar'\n",
    "df.loc[(df['job'] == 'unknown') & (df['education']=='basic.6y'), 'job'] = 'blue-collar'\n",
    "df.loc[(df['job'] == 'unknown') & (df['education']=='basic.9y'), 'job'] = 'blue-collar'\n",
    "df.loc[(df['job']=='unknown') & (df['education']=='professional.course'), 'job'] = 'technician'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-garlic",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pdays'] = np.where(df['pdays'] == 999, df[df['pdays'] < 999]['pdays'].mean(), df['pdays'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-namibia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
